## 1 Overview of RS and GNN

## 1.1 RS

**General Recommendation：**

- 认为用户具有静态的兴趣偏好，并根据隐式或显式反馈建模用户和物品间的匹配程度。从图的角度，user-item交互可以看做是二部图，GNN可以捕捉user-item交互，并学习user和item表示。另外，边信息也可以用来提升推荐性能，常见的策略是增加正则项或者融合边信息的表示。

- Bipartite graph : 用户与项目进行交互的二部图。

- Social relationship：社交关系，主要是在user端对推荐进行加强
- Knowledge Graph：知识图谱，主要是在item端对推荐进行加强

**Sequential Recommendation：**

- 捕捉item序列中的序列化模式，为用户推荐下一个感兴趣的物品。主要有基于马尔科夫链(MC)、基于RNN、基于注意力和自注意力机制的方法。随着GNN的出现，一些工作将item序列转换为图结构并用GNN捕捉其中的转移模式。

- Item-item transitions，


## 1.2 GNN

- GCN
- GraphSage
- GAT
- GGNN

### 1.3 Why GNN for Recommendation？

1. 大部分推荐问题中的数据是图结构的数据
2. GNN擅长捕捉节点间的连接和图数据的表示学习
3. 图神经网络相对于传统方法
   - 可能的优势：
     - 比较好的初始化效果：由于图神经网络的aggrgation操作如sum、mean节点的操作对节点的邻居节点有一个较好的信息统计能力，相比于传统的MF算法有较好的初始化能力。
     - 图网络的每一层信息累积不会忘记之前的训练数据
   - 可能的缺点：
     - 图网络的每一层信息累积不会忘记之前的训练数据，如果数据中有噪音，那么可能会出错。

## 2 General Recommendation

#### 2.1 Without side information：无边信息

该视角下推荐系统的关键问题是矩阵补全。从图的角度，矩阵补全可以看做是图上的链接预测(link prediction)问题。GNN可以捕捉高阶交互，就像局部的结构信息，在二部图上应用GNN就是**利用用户交互过的物品增强用户表示**，对item同理。
##### 2.1.1 总体框架
  - 构图：考虑计算效率，如何采样邻居节点；
  - 邻居聚合：如何从邻居聚合信息，多少信息应该被传播；
  - 信息更新：如何整合中心节点的表示和从邻居聚合来的表示；
  - 最终节点表示：用最后一层的表示还是结合所有层的结点表示
##### 2.1.2 代表性的方法
  - GC-MC
      - 主要面对链接预测的问题，将用户与项目的交互重构为一个二部图，图上的边的信息表示用户对项目的评分。用户节点被表示为之前所有交互的项目节点的embedding，项目节点被表示为之前被用户交互过的节点的embedding。他假设不同的item对用户的影响是相同的，因为这个模型使用的是mean-poolling。
      - GCMC只考虑了一跳邻居，没有充分考虑图的结构信息。
      - 丢弃节点本来的信息可能会丢失点的固有属性信息。
  - STAR-GCN ：Stacked and Reconstructed Graph Convolutional Networks for Recommender Systems
      - 是一系列的GCN块堆积，每个GCN是等价的。这样操作的动机在于，如果只是设置一个GCN块，但是这个GCN有很多层，往往会引起过平滑的问题。
      - STAR-GCN介绍了一种重构机制，用聚合的表示生成最后的重构表示，并且重构表示之后变为下一层的输入。
      - STAR-GCN利用重建策略来缓解过度平滑的问题，并共同优化预测任务和重建任务，与GC-MC相比，其性能更高。 GC-MC和STAR-GCN平等对待邻居的影响，并且从邻居传播的消息仅取决于邻居节点。 他们两个都只使用节点表示的最后一层进行等级预测。
  - NGCF
      - 本文的主要创新点在于**利用二部图神经网络将 User-Item 的历史交互信息编码进 Embedding 进而提升推荐效果。**更重要的是，本文显式地考虑 User-Item 之间的高阶连接性来进一步提升 embedding 的表示能力。
      - MF的输入单一，不能使用用户与物品的特征作为输入；同时MF对稀疏的交互矩阵效果较差。NGCF作者认为使用GNN能同时解决这两个问题。1. GNN使用节点特征作为初始embedding，可以在这里加入用户特征、物品特征；2. GNN能够引入『高阶交互历史信息』。
  - PinSage
      - 结合随机游走和图卷积学习节点嵌入。设计了一个基于随机游走的采样方法采样固定大小的邻居，计算结点的l1-norm访问计数，并取topT邻居节点。由于邻居节点是基于重要性采样出来的，相当于聚合邻居表示时也考虑了邻居的重要性。
  - IG-MC
      - 是一种归纳式(inductive)矩阵补全模型，为每个user-item对构建1跳包含子图(enclosing subgraph)，并用GNN训练由子图映射到评分的回归模型。抽取包含子图减少了对原始图结构的依赖，缓解了稀疏性造成的性能下降，增强了泛化能力，能够将模型迁移到其他数据集。与GC-MC的传播策略不同，IG-MC保留了中心节点的一些信息。模型也聚合了不同层的信息作为最终的表示。

2.1.3 总结

- 图建构问题：

  - GNN逐层迭代地传播来自单跳邻居的消息。 节点的大小随层数呈指数增长。 要将GNN应用于大型图，必须对邻域进行采样。
  - PinSage采用随机游走策略对固定大小的邻域进行采样。 IG-MC使用目标用户/项目的一跳邻居构造子图。该模型的性能取决于采样策略，更有效的采样策略值得进一步研究。
- 邻居聚合：

  - 聚合函数可以分为四类：

    - “mean pooling”对邻居的处理是一视同仁的。
    -  “degree normalization”基于图结构将权重分配给节点；
    -  “attentive pooling”通过注意力机制来区分邻居的重要性
    - “central node augmentation”则考虑节点之间的亲缘关系，并使用中央节点来过滤邻居的消息。 
  - 与平均池或度归一化相比，区分邻居的影响倾向于提高性能。 原因是，交互的项目不能平等地代表用户的喜好。
- 信息更新：
  - 使用邻居的聚合作为新的中心节点的表示，大多数的工作都结合了原来的表示。方法比如求平均、求和等等
- 最后表示：
  - 一些作品将GNN最后一层中的节点向量用作最终表示，而另一些作品则将所有层的表示与加权池或串联操作相结合。 由于不同层的输出表示不同的连接，因此利用所有层的表示将更有可能表现更好。

#### 2.2 在用户端用社交网络强化( Social network enhanced)：

- 社交网络是一个同质图，先前的工作通过增加社交正则项限制社交朋友具有相似的表示；或结合朋友的表示得到最终的用户表示。由于社交影响在社交网络中递归地传播和扩散，这符合GNN迭代传播的特征，所以最近的工作用GNN来模拟用户是如何被社交扩散过程影响的。
  - 特点：
    - 人与人之间联系有不同的紧密程度，边表示社交关系，但不知道关系强弱
    - 能够增强user-item交互，从社交图学到的知识可以增强用户表示
  - 问题：
    - **朋友影响**：是否有不同的影响，如何区别这些影响；
    - **偏好整合**：如何整合社交影响和交互行为，存在两种策略，一种交互图和社交图分别建模，一种将u-i交互和社交关系融合到一个图中统一建模。
##### 2.2.1 总体框架

现有研究将社会影响纳入推荐系统中，以利用用户之间的交互行为和关系，并改善其性能。社交图具有两个特殊特征：

（1）用户之间的边缘代表社交关系，其中一些社交关系很强，有些则很弱。但是，社会关系的优势始终是未知的。

（2）对于推荐系统，它补充了用户与项目之间的交互作用，即从社交图中学到的知识有助于增强用户表示。

对应于社会图的特征，有两个主要问题要处理：

- 朋友的影响朋友有同等的影响力吗？如果没有，如何区分不同朋友的影响力？
- 偏好整合用户与朋友有社交关系，并且也与商品互动。如何从社会影响力角度和交互行为整合用户表示？

##### 2.2.2 代表性的方法

  - DiffNet
      - 根据用户的社交关系和历史行为来模拟用户的偏好。 它采用graphSage框架来模拟用户如何受到递归社会扩散过程的影响。 基于朋友具有同等影响力的假设，DiffNet使用均值池功能来汇总朋友的表示。
  - GraphRec
      - 考虑到用户参与了社交网络图和用户项目二分图，并分别在这两个图中学习了具有图注意力网络的用户/项目嵌入。 利用串联操作的注意力机制来区分邻居的影响。 在社交图中，朋友的影响取决于他们潜在向量之间的相似性。
  - DANSER
      - 考虑了用户与用户之间的社会关系以及项目与项目之间的关系。与大多数现有作品假定朋友的社交效应是静态的不同，DANSER利用图注意力网络来协作学习两种社交效应的表示形式，其中一种是通过特定于用户的注意力权重建模的，另一种是通过动态和上下文建模的注意力权重。
  - DiffNet++
      - 在一个统一的框架下建模影响力扩散和兴趣扩散。对于用户节点，首先利用GAT在二部图和神经网络上聚合邻居信息，注意力机制被用来混合邻居的两种表示，用户节点通过与混合向量相加来更新。对于item节点，利用GAT传播交互邻居的信息。为捕捉不同深度的社交关系，该模型拼接不同层的表示作为最终的表示。模型优势在于两个扩散过程，不同层拼接，以及注意力机制。

2.2.3 总结

影响力建模：利用注意力机制来区分朋友的不同影响比mean-pooling方式表现更好，另外来自朋友的影响力也受item的影响，因此DANSER建模动态的朋友表示。

偏好整合：该场景下存在两种类型的图，有两种策略融合两个网络：一种分别建模两个图，优势是此时邻居都是同质的；另一种是在一个统一的图上建模，优势在于用户的表示能够同时依赖两种信息进行更新。哪种策略更好，暂无定论。

#### 2.3 在Item端用知识图谱强化Knowledge graph enhanced

  - 好处：
    - KG中的项目之间具有丰富的语义关联性，这种性质有助于发掘item之间的潜在联系并提高结果的准确性
    - 合理地扩展用户的兴趣并增加推荐项目的多样性
    - KG将用户的历史喜欢和推荐的商品联系起来，从而为推荐系统带来了可解释性。
  - 问题：
    - 这个知识图谱是一个复杂的图结构，含有多种类型的实体和关系，存在三个问题需要解决：
    - 图简化：基于最短路径算法重建子图
    - 多关系传播：广泛采用了注意机制来汇总邻居的信息
    - 用户整合(将用户角色引入图结构)
##### 2.3.1 总体框架

 KG能够捕捉item之间的联系从而更好建模用户对item的偏好。存在三个问题需要解决：1）图简化；2）多关系传播；3）用户整合(将用户角色引入图结构)。

##### 2.3.2 代表性的方法
  - KGCN
      - 利用user-specific relation-aware GNN聚合邻居的实体信息。该工作利用知识图得到语义感知的item表示，出于计算有效性考虑，为每个实体采样固定大小的邻居，将用户偏好表示为由隐向量组成的集合，该集合也可以用来衡量用户在不同关系上的兴趣。利用user-specific relation-aware GNN聚合邻居的实体信息。该工作利用知识图得到语义感知的item表示，出于计算有效性考虑，为每个实体采样固定大小的邻居，将用户偏好表示为由隐向量组成的集合，该集合也可以用来衡量用户在不同关系上的兴趣。
  - KGNN-LS
      - 旨在通过为给定用户识别重要的KG中关系来学习用户特定的item embedding。模型通过关系打分函数计算关系对用户的重要性，此时KG中的多种关系可以表示为用户特定的邻接矩阵，此时可以将复杂图看作传统图，可使用GCN。
  - KGAT
  - AKGE

**2.3.3 总结**

图简化：多种类型的实体和关系为应用GNN进行表示学习带来挑战，为此，一些工作会简化图结构，例如AKGE基于最短路径算法重构子图，IntentGC保留user-to-user关系和item-to-item关系分别构建用户和物品子图。图简化能够在损失一些图信息的前提下提高计算效率。

多关系传播：知识图谱的一大特性就是多关系。注意力机制广泛用于区分不同关系的重要性，注意力函数能够决定信息如何传播。多数模型考虑节点和关系为邻居加权。

用户整合：KG可看作是二部图以外的边信息，但是KG的规模更大。一些工作假设用户有静态的表示来应用GNN学习item表示；或者融合两种图，将用户看作是KG中的一种实体。

#### 2.4 序列性问题的推荐SEQUENTIAL RECOMMENDATION

Session-based Recommendation with Graph Neural Networks
根据用户最近感兴趣的项目对用户之后感兴趣的item进行预测，这种会话的预测基本都是对当时给出的信息进行预测。

  - 问题：
    - 构图
    - 信息传播
    - 序列化偏好。

##### 2.4.1 总体框架

##### 2.4.2 代表性的方法

  - KGCN
  - KGNN-LS
  - KGAT
  - AKGE

